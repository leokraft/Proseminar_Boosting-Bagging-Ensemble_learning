# Proseminar - Boosting, Bagging and Ensemble learning

This seminary paper has been written for the course "Proseminar: Advanced Topics in Machine Learning" at Karlsruhe Institute of Technology (KIT).
The full document can be found [here](https://raw.githubusercontent.com/leokraft/Proseminar_Boosting-Bagging-Ensemble_learning/main/seminary_paper/Proseminar_Boosting_Bagging_and_Ensemble_learning.pdf).

## Abstract

Ensemble methods can be used to combine multiple learners to form a single,
stronger learner. Across a wide range of fields, from financial to medical
applications, machine learning is often applied in situations where maximum
robustness and accuracy play a crucial role. This paper gives an overview of
two ensemble methods - boosting and bagging - and shows how these methods
can be utilized. Various choices of voting, boosting, and bagging methods are
covered. An additional focus is also put on the common Random Forests and
AdaBoost methods. Finally, a guideline on when to use which ensemble method
is provided to guide future decisions on which methods to choose.

## Running the code

To run the provided code you need the dependencies listed inside the `requirements.txt`.\
These can be installed automatically by running:

```shell
pip install -r requirements.txt
```
