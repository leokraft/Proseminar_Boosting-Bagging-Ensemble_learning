% This file was created with Citavi 6.10.0.0

@article{RobertE.Schapire.1999,
    author = {{Robert E. Schapire}},
    year = {1999},
    title = {A Brief Introduction to Boosting},
    file = {10.1.1.640.9942:Attachments/10.1.1.640.9942.pdf:application/pdf}
}

@article{RichardMaclin.,
    author = {{Richard Maclin}, David Opitz},
    title = {An Empirical Evaluation of Bagging and Boosting},
    url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.6964&rep=rep1&type=pdf},
    year = {1997}
    urldate = {20/07/2021},
    file = {paper:Attachments/paper.pdf:application/pdf}
}

@article{Ju.2018,
    abstract = {Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies fully investigated ensembles of artificial neural networks. In this work, we investigated multiple widely used ensemble methods, including unweighted averaging, majority voting, the Bayes Optimal Classifier, and the (discrete) Super Learner, for image recognition tasks, with deep neural networks as candidate algorithms. We designed several experiments, with the candidate algorithms being the same network structure with different model checkpoints within a single training process, networks with same structure but trained multiple times stochastically, and networks with different structure. In addition, we further studied the over-confidence phenomenon of the neural networks, as well as its impact on the ensemble methods. Across all of our experiments, the Super Learner achieved best performance among all the ensemble methods in this study.},
    author = {Ju, Cheng and Bibaut, Aur{\'e}lien and {van der Laan}, Mark},
    year = {2018},
    title = {The Relative Performance of Ensemble Methods with Deep Convolutional Neural Networks for Image Classification},
    url = {https://www.tandfonline.com/doi/pdf/10.1080/02664763.2018.1441383},
    urldate = {20/07/2021},
    pages = {2800--2818},
    volume = {45},
    number = {15},
    issn = {0266-4763},
    journal = {Journal of applied statistics},
    doi = {10.1080/02664763.2018.1441383},
    file = {The relative performance of ensemble methods with deep convolutional neural networks for image classification - 02664763.2018:Attachments/The relative performance of ensemble methods with deep convolutional neural networks for image classification - 02664763.2018.pdf:application/pdf}
}

@article{Breiman.1996,
    author = {Breiman, Leo},
    year = {1996},
    title = {Bagging predictors},
    url = {https://link.springer.com/content/pdf/10.1007/BF00058655.pdf},
    urldate = {20/07/2021},
    pages = {123--140},
    volume = {24},
    number = {2},
    issn = {0885-6125},
    journal = {Machine Learning},
    doi = {10.1007/BF00058655},
}

@book{Freund.1997,
    author = {Freund, Yoav and Schapire, Robert E.},
    year = {1997},
    title = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
    url = {https://reader.elsevier.com/reader/sd/pii/S002200009791504X},
    volume = {55},
    doi = {10.1006/jcss.1997.1504},
    file = {Freund, Schapire 1997 - A Decision-Theoretic Generalization of On-Line:Attachments/Freund, Schapire 1997 - A Decision-Theoretic Generalization of On-Line.pdf:application/pdf}
}

% This file was created with Citavi 6.10.0.0

@article{Opitz.1999,
    abstract = {An ensemble consists of a set of individually trained    classifiers (such as neural networks or decision trees) whose    predictions are combined when classifying novel instances.  Previous    research has shown that an ensemble is often more accurate than any of    the single classifiers in the ensemble.  Bagging (Breiman, 1996c) and    Boosting (Freund {\&} Shapire, 1996; Shapire, 1990) are two relatively    new but popular methods for producing ensembles.  In this paper we    evaluate these methods on 23 data sets using both neural networks and    decision trees as our classification algorithm.  Our results clearly    indicate a number of conclusions.  First, while Bagging is almost    always more accurate than a single classifier, it is sometimes much    less accurate than Boosting.  On the other hand, Boosting can create    ensembles that are less accurate than a single classifier --    especially when using neural networks.  Analysis indicates that the    performance of the Boosting methods is dependent on the    characteristics of the data set being examined.  In fact, further    results show that Boosting ensembles may overfit noisy data sets, thus    decreasing its performance.  Finally, consistent with previous    studies, our work suggests that most of the gain in an ensemble's    performance comes in the first few classifiers combined; however,    relatively large gains can be seen up to 25 classifiers when Boosting    decision trees.},
    author = {Opitz, D. and Maclin, R.},
    year = {1999},
    title = {Popular Ensemble Methods: An Empirical Study},
    url = {https://www.jair.org/index.php/jair/article/view/10239},
    pages = {169--198},
    volume = {11},
    issn = {1076-9757},
    journal = {Journal of Artificial Intelligence Research},
    doi = {10.1613/jair.614},
    file = {Opitz, Maclin 1999 - Popular Ensemble Methods:Attachments/Opitz, Maclin 1999 - Popular Ensemble Methods.pdf:application/pdf}
}

@article{LEOBREIMAN.,
    author = {{Leo Breiman}},
    year = {1999},
    title = {Random Forests},
    url = {https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf},
    urldate = {20/07/2021},
    file = {Random Forests - Breiman2001{\_}Article{\_}RandomForests:Attachments/Random Forests - Breiman2001{\_}Article{\_}RandomForests.pdf:application/pdf}
}

@misc{xgboost,
    booktitle = {xgboost},
    publisher = {Distributed (Deep) Machine Learning Community},
    title = {{Machine Learning Challenge Winning Solutions}},
    url = {https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions},
    urldate = {2022-03-31},
    howpublished = {GitHub}
}